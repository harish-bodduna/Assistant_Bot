{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a06ae5-109f-4f50-b530-fc876ac354e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azure-storage-blob>=12.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cce502-fa23-4672-bd61-d3c684c65421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad9f26-a397-4886-81b3-95b9a809f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2153515e-b490-4084-9042-ac8df2481e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "import uuid\n",
    "import pytesseract\n",
    "import secrets\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling_core.types.doc import PictureItem, TextItem\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c14d410-4c34-4e9e-8faa-7a8e7b4218f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc2597e-179f-4da6-a85a-97dc9ab7f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09eb59d6-25cc-4cc8-a993-a257a987e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def upload_and_get_sas(data, blob_path):\n",
    "    # 1. Get blob client and upload\n",
    "    service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    blob_client = service_client.get_blob_client(container='dummy', blob=blob_path)\n",
    "    blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "    # 2. Generate SAS token valid for (e.g.) 1 year\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=service_client.account_name,\n",
    "        container_name='dummy',\n",
    "        blob_name=blob_path,\n",
    "        account_key=service_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(days=365)\n",
    "    )\n",
    "\n",
    "    # 3. Return the full URL\n",
    "    return f\"{blob_client.url}?{sas_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f29b8a1-0e9f-4372-9538-12de81636e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.generate_picture_images = True\n",
    "pipeline_options.images_scale = 2.0\n",
    "pipeline_options.generate_page_images=True\n",
    "pipeline_options.do_ocr=True\n",
    "pipeline_options.do_table_structure=True\n",
    "pipeline_options.generate_parsed_pages=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4466cfef-7269-470f-adc4-4c13cc9ee996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pass these options to the converter\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620fbd7e-2acd-4be0-9bad-9436fd26cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"C:\\Users\\Harish\\Workspace\\maestro_projects\\1440_Bot\\source_docs\\1440-Microsoft Multifactor Authentication Documentation.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d540b9-0141-4ae5-8439-9b7508503d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 16:21:26,877 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-14 16:21:26,911 - INFO - Going to convert document batch...\n",
      "2026-01-14 16:21:26,912 - INFO - Initializing pipeline for StandardPdfPipeline with options hash c00cd68e79f4403162cfad1c1430e139\n",
      "2026-01-14 16:21:26,961 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-14 16:21:26,963 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-14 16:21:27,058 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-14 16:21:27,062 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-14 16:21:27,504 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2026-01-14 16:21:27,505 - INFO - easyocr cannot be used because it is not installed.\n",
      "2026-01-14 16:21:27,788 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:27,809 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:27,817 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:27,827 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Harish\\anaconda3\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:27,828 [RapidOCR] main.py:50: Using C:\\Users\\Harish\\anaconda3\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,335 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,336 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,339 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Harish\\anaconda3\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,340 [RapidOCR] main.py:50: Using C:\\Users\\Harish\\anaconda3\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,442 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,443 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,461 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Harish\\anaconda3\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-14 16:21:28,462 [RapidOCR] main.py:50: Using C:\\Users\\Harish\\anaconda3\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2026-01-14 16:21:28,731 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2026-01-14 16:21:28,824 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-14 16:21:28,829 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-14 16:21:32,272 - INFO - Accelerator device: 'cpu'\n",
      "2026-01-14 16:21:33,463 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-14 16:21:33,464 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-14 16:21:33,572 - INFO - Accelerator device: 'cpu'\n",
      "2026-01-14 16:21:33,981 - INFO - Processing document 1440-Microsoft Multifactor Authentication Documentation.pdf\n"
     ]
    }
   ],
   "source": [
    "# 3. Proceed with your conversion and iteration\n",
    "result = converter.convert(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f8f64-1573-4c0b-b28d-d314fa9f9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48564fce-2e43-4a95-8998-73f69dac97f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(doc.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58324ac2-96f5-4a1a-b80b-008bbca8cf05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"C:/Users/Harish/Workspace/maestro_projects/1440_Bot/hash_images\"\n",
    "\n",
    "def build_reference_hashes(ref_dir):\n",
    "    ref_hashes = []\n",
    "    # Supported formats\n",
    "    valid_exts = (\".png\", \".jpg\", \".jpeg\")\n",
    "    \n",
    "    for filename in os.listdir(ref_dir):\n",
    "        if filename.lower().endswith(valid_exts):\n",
    "            path = os.path.join(ref_dir, filename)\n",
    "            with Image.open(path) as img:\n",
    "                # We use dhash here as it's more robust for icons/symbols\n",
    "                h = imagehash.phash(img.convert(\"RGB\"))\n",
    "                ref_hashes.append(h)\n",
    "                print(f\"Added to Banned List: {filename} (Hash: {h})\")\n",
    "    return ref_hashes\n",
    "\n",
    "BANNED_REF_LIST = build_reference_hashes(IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31028ab-58c5-4947-a991-8f69c5fbe47e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collected = []\n",
    "seen_hashes = set()\n",
    "PHASH_THRESHOLD = 15\n",
    "\n",
    "\n",
    "# Iterate items in the exact order they appear in the PDF\n",
    "for idx, (element, _level) in enumerate(doc.iterate_items()):\n",
    "    if isinstance(element, TextItem):\n",
    "        text = (element.text or \"\").strip()\n",
    "        if text:\n",
    "            collected.append({\"type\": \"text\", \"content\": text})\n",
    "    \n",
    "    elif isinstance(element, PictureItem):\n",
    "        if element.image and element.image.pil_image:\n",
    "            pil_img = element.image.pil_image\n",
    "            \n",
    "            # 2. Generate hash for filtering\n",
    "            try:\n",
    "                curr_hash = imagehash.phash(pil_img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error hashing image at index {idx}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # 3. Filter against BANNED_REF_LIST (Logos/Arrows/Icons)\n",
    "            is_banned = any((curr_hash - ref) <= PHASH_THRESHOLD for ref in BANNED_REF_LIST)\n",
    "            \n",
    "            if is_banned:\n",
    "                print(f\"Skipping Image {idx}: Matched Banned Reference.\")\n",
    "                display(pil_img)\n",
    "                continue\n",
    "            \n",
    "            # # 4. Filter against duplicates within the same document\n",
    "            # is_duplicate = any((curr_hash - h) < PHASH_THRESHOLD for h in seen_hashes)\n",
    "            # if is_duplicate:\n",
    "            #     print(f\"Skipping Image {idx}: Duplicate found in document.\")\n",
    "            #     display(pil_img)\n",
    "            #     continue\n",
    "            \n",
    "            # 5. Add to seen hashes and collect the clean image\n",
    "            seen_hashes.add(curr_hash)\n",
    "            \n",
    "            print(f\"Keeping Image {idx}: Valid technical asset\")\n",
    "            display(pil_img)\n",
    "            \n",
    "            collected.append({\n",
    "                \"type\": \"image\",\n",
    "                \"image\": pil_img, \n",
    "                \"index\": idx,\n",
    "                \"page\": element.prov[0].page_no if element.prov else 0,\n",
    "                \"bbox\": element.prov[0].bbox if element.prov else None\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337fa0a-9666-4efa-90af-44d2012c2de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Containers for both versions\n",
    "ocr_structural_markdown = [] # Version A: OCR Placeholders for reasoning\n",
    "final_sas_markdown = []      # Version B: The SAS URL version you want to keep\n",
    "asset_manifest_data = []      # The \"Bridge\" mapping table\n",
    "vision_sas_urls = []         # For the model's visual input\n",
    "\n",
    "project_name = \"dummy\"\n",
    "storage_base_path = f\"{project_name}/processed_images\"\n",
    "\n",
    "for item in collected:\n",
    "    if item[\"type\"] == \"text\":\n",
    "        content = item[\"content\"]\n",
    "        ocr_structural_markdown.append(f\"[DOC_TEXT]: {content}\")\n",
    "        final_sas_markdown.append(content)\n",
    "    \n",
    "    elif item[\"type\"] == \"image\":\n",
    "        pil_img = item[\"image\"]\n",
    "        page = item.get(\"page\", 0)\n",
    "        \n",
    "        # --- PHASE 1: Generate the Shared ID ---\n",
    "        # This ID ties the OCR text to the SAS URL permanently\n",
    "        unique_hex = ''.join(secrets.choice(string.hexdigits.lower()) for _ in range(4))\n",
    "        asset_id = f\"ASSET_{unique_hex}\"\n",
    "        \n",
    "        # --- PHASE 2: OCR Extraction ---\n",
    "        try:\n",
    "            ocr_text = pytesseract.image_to_string(pil_img).strip().replace(\"\\n\", \" \")\n",
    "        except Exception:\n",
    "            ocr_text = \"[No readable text found]\"\n",
    "\n",
    "        # --- PHASE 3: Original Storage Logic ---\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        pil_img.save(img_byte_arr, format='PNG')\n",
    "        img_bytes = img_byte_arr.getvalue()\n",
    "        \n",
    "        filename = f\"visual_{unique_hex}_page_{page}.png\"\n",
    "        blob_path = f\"{storage_base_path}/{filename}\"\n",
    "        \n",
    "        # Keep your existing upload function exactly as it is\n",
    "        sas_url = upload_and_get_sas(img_bytes, blob_path)\n",
    "\n",
    "        # --- PHASE 4: Populate Both Markdown Versions ---\n",
    "        \n",
    "        # A. The OCR Version (Placeholders for LLM reasoning)\n",
    "        img_placeholder = (\n",
    "            f\"\\n--- IMAGE OCR PLACEHOLDER ---\\n\"\n",
    "            f\"SOURCE_ID: {asset_id}\\n\"\n",
    "            f\"TEXT_FOUND_IN_IMAGE: {ocr_text}\\n\"\n",
    "            f\"--- END OCR PLACEHOLDER ---\\n\"\n",
    "        )\n",
    "        ocr_structural_markdown.append(img_placeholder)\n",
    "        \n",
    "        # B. The SAS URL Version (Interleaved images with clickable links)\n",
    "        # Using the unique_hex in alt-text for cross-referencing\n",
    "        final_sas_markdown.append(f\"![Document Visual {unique_hex}]({sas_url})\")\n",
    "\n",
    "        # C. The Manifest (The Source of Truth table)\n",
    "        asset_manifest_data.append({\n",
    "            \"id\": asset_id,\n",
    "            \"ocr\": ocr_text,\n",
    "            \"url\": sas_url\n",
    "        })\n",
    "        vision_sas_urls.append(sas_url)\n",
    "\n",
    "# Final Strings\n",
    "llm_reasoning_draft = \"\\n\".join(ocr_structural_markdown)\n",
    "llm_ready_sas_markdown = \"\\n\\n\".join(final_sas_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987ccff-a0d7-4361-a7bd-cfca9a497fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(llm_reasoning_draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92f14c-5bdf-48d4-a390-867066d29a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(llm_ready_sas_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352d531-754d-4e9c-b770-920bcc21b5e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# asset_manifest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf75f40-71da-4528-8fdb-8b827488d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision_sas_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a64527-453f-4f60-b36f-77a9fa1127b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d39a5-7b6f-4666-8ca0-4aa7d74bcdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d0196-4c04-4b68-9fe6-8da6d8d7de13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abc6a2-f986-4e53-825c-0a3a635d6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f73be-b0bb-49de-8bd8-9a1f9542f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = f\"Explain multifactor authentication steps?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b5260-613a-4f11-9a4c-ff4e50e504d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_system_prompt = (\n",
    "    \"\"\"# System Prompt: Multi-Modal Technical Documentation Assistant\n",
    "\n",
    "You are a technical documentation assistant that answers user queries by analyzing interleaved text, OCR data, and visual content from technical documents.\n",
    "\n",
    "## YOUR INPUT STRUCTURE\n",
    "\n",
    "You will receive four components:\n",
    "\n",
    "1. **USER QUERY**: The specific question to answer\n",
    "2. **IMAGE-TO-OCR MAPPING MANIFEST**: A lookup dictionary linking Asset IDs to OCR content and SAS URLs\n",
    "3. **SOURCE DOCUMENT DRAFT**: Text content with OCR placeholders\n",
    "4. **ACTUAL IMAGES**: The visual content to verify and understand context\n",
    "\n",
    "## YOUR CORE TASK\n",
    "\n",
    "Answer the user's query by:\n",
    "1. **Semantic matching** between text instructions and visual content using OCR hints\n",
    "2. **Logical reconstruction** of steps in proper sequence (Step 1 → 2 → 3...)\n",
    "3. **Visual verification** using the actual images to confirm context\n",
    "4. **Precise image binding** to ensure each step shows the correct screenshots\n",
    "\n",
    "## CRITICAL IMAGE HANDLING RULES\n",
    "\n",
    "### URL Fidelity (NON-NEGOTIABLE)\n",
    "- Use `FINAL_SAS_URL` from the manifest EXACTLY as provided\n",
    "- **NEVER modify, truncate, or regenerate these URLs**\n",
    "- Include the complete signature: `?se=...&sp=...&sig=...`\n",
    "- Format: `![Description](full_URL_with_all_parameters)`\n",
    "\n",
    "### Semantic Matching Process\n",
    "1. Read the step description from [DOC_TEXT] (e.g., \"Scan QR code\")\n",
    "2. Check the manifest's `OCR_CONTENT` for matching keywords\n",
    "3. Look at the actual image to verify it matches the description\n",
    "4. Bind the correct `FINAL_SAS_URL` to that step\n",
    "\n",
    "### Multi-Image Steps\n",
    "- If multiple images relate to one step, include all relevant URLs sequentially\n",
    "- Example: Step 4 might show 3 app screens - include all 3 with their exact URLs\n",
    "\n",
    "## RESPONSE STRUCTURE\n",
    "\n",
    "### For Procedural Queries (e.g., \"Explain MFA steps\")\n",
    "\n",
    "Provide a brief overview, then break down each step:\n",
    "\n",
    "**Step 1: [Action Title]**\n",
    "Clear description of what the user should do.\n",
    "\n",
    "![Description of what this shows](https://exact_sas_url_from_manifest...)\n",
    "\n",
    "**Step 2: [Action Title]**\n",
    "Clear description of the next action.\n",
    "\n",
    "![Description of what this shows](https://exact_sas_url_from_manifest...)\n",
    "\n",
    "### For FAQ Queries\n",
    "Quote the relevant FAQ section and include any associated images with their exact URLs.\n",
    "\n",
    "### For Specific Questions\n",
    "Provide a direct answer using only information from the source material, with relevant images.\n",
    "\n",
    "## LOGICAL RECONSTRUCTION RULES\n",
    "\n",
    "The source draft may have steps out of order due to multi-column PDF parsing. You must:\n",
    "\n",
    "1. **Identify all numbered steps** (Step One, Step Two, Step Three, etc.)\n",
    "2. **Reorder them numerically** regardless of their position in the draft\n",
    "3. **Match images semantically** using OCR content, not just position in document\n",
    "4. **Verify with your vision** by examining what the images actually show.\n",
    "5. **Order** If a step refers to an action shown in an image (e.g., 'Click the Accept button'), ensure the image containing that specific button is the one placed under that step.\n",
    "\n",
    "**Example**: If the draft shows \"Step Four\" before \"Step Three\", but Step Three discusses permissions and you see an image with \"Permissions requested\" in it, that image belongs to Step Three.\n",
    "\n",
    "## CONTENT QUALITY RULES\n",
    "\n",
    "- **Remove parsing artifacts**: Strip `[DOC_TEXT]:`, `--- IMAGE OCR PLACEHOLDER ---`, `SOURCE_ID:`, etc.\n",
    "- **Deduplicate**: Remove repeated \"FAQ's\" headers or redundant sections\n",
    "- **Clean formatting**: Use proper markdown with bold for step titles\n",
    "- **Be concise**: Answer the query directly, don't reconstruct the entire document unless asked\n",
    "\n",
    "## BOUNDARIES & CONSTRAINTS\n",
    "\n",
    "- **Use only provided information** - no external knowledge about products/services\n",
    "- **If information is missing**: State \"This detail is not available in the documentation\"\n",
    "- **For unanswerable queries**: Provide the fallback contact from the document\n",
    "- **Trust what you see**: If OCR hints and actual image content conflict, trust the image\n",
    "- **Missing images**: If a step mentions an image but you can't find it in the manifest, write \"Visual not available\"\n",
    "\n",
    "## OUTPUT REQUIREMENTS\n",
    "\n",
    "- Answer the user's query directly and professionally\n",
    "- Include relevant images with their EXACT URLs from the manifest\n",
    "- Use clear, structured formatting\n",
    "- No meta-commentary about your reasoning process\n",
    "- No explanations of how you processed the data\n",
    "- Focus on being helpful and accurate\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5083645-ff34-48e1-8825-20995528107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_rules = (\n",
    "    \"\"\"\"1. ANALYSIS: Compare the instruction steps in [DOC_TEXT] against the [OCR_CONTENT] in the Manifest.\n",
    "    2. ANCHORING: If 'Step Three' mentions 'Permissions' and an image OCR contains 'Permissions requested\n",
    "    3.\"move that image URL to Step Three, regardless of where it appeared in the draft.\n",
    "    4. LOGICAL ORDERING: Ensure the final output follows a strict numerical sequence (Step 1, 2, 3, ....).\n",
    "    5. IMAGE PLACEMENT: Place the image(s) IMMEDIATELY after the step description.\n",
    "    6. FIDELITY: Maintain the SAS URLs exactly as provided in the manifest. Do not truncate or modify them\n",
    "    7. DEDUPLICATION: Remove any redundant Step text or duplicate FAQ sections found in the draft.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e0577-1be9-45e0-beb8-38e121fb9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the content array with text and images\n",
    "content = []\n",
    "\n",
    "# 1. User query\n",
    "content.append({\n",
    "    \"type\": \"input_text\",\n",
    "    \"text\": f\"### USER QUERY\\n{user_query}\\n\\n\"\n",
    "})\n",
    "\n",
    "# 2. Image manifest\n",
    "content.append({\n",
    "    \"type\": \"input_text\",\n",
    "    \"text\": f\"### IMAGE-TO-OCR MAPPING MANIFEST\\n{asset_manifest_data}\\n\\n\" \n",
    "})\n",
    "\n",
    "# 3. Source document draft\n",
    "content.append({\n",
    "    \"type\": \"input_text\",\n",
    "    \"text\": f\"### SOURCE DOCUMENT DRAFT\\n{llm_reasoning_draft}\\n\" \n",
    "})\n",
    "\n",
    "# 4. Add all images\n",
    "for image_url in vision_sas_urls:\n",
    "    content.append({\n",
    "        \"type\": \"input_image\",\n",
    "        \"image_url\": image_url\n",
    "    })\n",
    "\n",
    "# # 5. Reconstruction rules (optional, could be in system prompt)\n",
    "# content.append({\n",
    "#     \"type\": \"text\",\n",
    "#     \"text\": reconstruction_rules_text\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cec9c-e813-44ee-a0f7-34655a73214b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    \n",
    "    model=\"gpt-5.2\",  # required in practice\n",
    "    instructions=full_system_prompt,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    reasoning={\n",
    "        \"effort\": \"high\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cecc5-7f3d-4aaa-beed-c1071cec7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca583aa-7076-44e8-b444-5ccb163a9259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa8415-7b0c-4075-a271-06f2e8c232a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19641cb7-4550-4082-a013-ff44067900fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ddee8-69e0-4179-8894-307bc2b6615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_system_prompt = (\n",
    "#     \"\"\"# System Prompt: Technical Documentation Assistant\n",
    "# You are a technical documentation assistant that helps users understand procedures and information from provided documentation.\n",
    "\n",
    "# ## Core Responsibility\n",
    "# Answer user queries using ONLY the information in the provided markdown document. The document contains text instructions and embedded image SAS URLs that illustrate those instructions.\n",
    "\n",
    "# ## Critical Rules for Image Handling\n",
    "\n",
    "# ### Image URL Preservation\n",
    "# - The markdown contains image SAS URLs in this format: `![Description](https://url...?signature...)`\n",
    "# - **NEVER modify, truncate, or alter these URLs in any way**\n",
    "# - Copy them EXACTLY as provided, including all query parameters and signatures\n",
    "# - These URLs are time-sensitive and any modification will break them\n",
    "\n",
    "# ### Image-Text Association\n",
    "# - When explaining a step or concept, include the relevant image SAS URL(s) that illustrate it\n",
    "# - Place the image or images reference immediately after describing what it shows\n",
    "# - If multiple images relate to one explanation, include all relevant images in sequence\n",
    "# - Format example:\n",
    "#   ```\n",
    "#   **Step 2: Enter Credentials**\n",
    "#   Provide your username and password on the login screen.\n",
    "  \n",
    "#   ![Visual showing login screen](https://full_url_exactly_as_provided...)\n",
    "#   ```\n",
    "\n",
    "# ## Response Structure\n",
    "\n",
    "# 1. **Direct answer first** - Start with a brief response to the user's question\n",
    "# 2. **Sequential explanation** - Break down procedures step-by-step in logical order\n",
    "# 3. **Visual references** - Include relevant images inline with explanations\n",
    "\n",
    "# ## Content Boundaries\n",
    "\n",
    "# - **Only use information from the provided documentation** - Never add external knowledge\n",
    "# - **If information is missing or unclear**, state: \"This information is not available in the provided documentation\"\n",
    "# - **If contact information exists in the document**, provide it when you cannot answer a query\n",
    "# - **Never invent steps, details, or procedures** not explicitly in the source material\n",
    "\n",
    "# ## Tone\n",
    "\n",
    "# - Professional and helpful\n",
    "# - Clear and concise\n",
    "# - Action-oriented for procedural content\n",
    "# - Factual without speculation\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c33944-e36a-463d-84f1-371270fbce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Format the asset_manifest_data into a clear lookup table for the LLM\n",
    "# manifest_reference_table = \"### IMAGE-TO-OCR MAPPING MANIFEST\\n\"\n",
    "# manifest_reference_table += \"Use this table to match the textual instructions to the correct Image Asset ID and URL based on the OCR content.\\n\\n\"\n",
    "\n",
    "# for asset in asset_manifest_data:\n",
    "#     manifest_reference_table += (\n",
    "#         f\"- ID: {asset['id']}\\n\"\n",
    "#         f\"  OCR_CONTENT: \\\"{asset['ocr']}\\\"\\n\"\n",
    "#         f\"  FINAL_SAS_URL: {asset['url']}\\n\"\n",
    "#     )\n",
    "\n",
    "# api_content = []\n",
    "\n",
    "# # 1. Lead with the User's Query\n",
    "# api_content.append({\n",
    "#     \"type\": \"input_text\", \n",
    "#     \"text\": f\"### USER QUERY / INTENT\\n{user_query}\\n\\n\"\n",
    "# })\n",
    "\n",
    "# # 2. Provide the Source of Truth (The Manifest)\n",
    "# # This is the \"dictionary\" that ties OCR meaning to specific SAS URLs\n",
    "# api_content.append({\n",
    "#     \"type\": \"input_text\", \n",
    "#     \"text\": manifest_reference_table\n",
    "# })\n",
    "\n",
    "# # 3. Provide the Reasoning Draft (The interleaved version with placeholders)\n",
    "# # This gives the model the narrative flow it needs to fix\n",
    "# api_content.append({\n",
    "#     \"type\": \"input_text\", \n",
    "#     \"text\": f\"### SOURCE DOCUMENT DRAFT (REASONING VERSION):\\n{llm_reasoning_draft}\"\n",
    "# })\n",
    "\n",
    "# # 4. Provide the High-Res Vision Inputs\n",
    "# # This allows the model to visually verify the OCR and see UI elements\n",
    "# for url in vision_sas_urls:\n",
    "#     api_content.append({\n",
    "#         \"type\": \"input_image\",\n",
    "#         \"image_url\": url\n",
    "#     })\n",
    "\n",
    "# # 5. Reasoning & Output Instructions\n",
    "# api_content.append({\n",
    "#     \"type\": \"input_text\",\n",
    "#     \"text\": (\n",
    "#         \"\\n--- RECONSTRUCTION RULES ---\\n\"\n",
    "#         \"1. ANALYSIS: Compare the instruction steps in [DOC_TEXT] against the [OCR_CONTENT] in the Manifest.\\n\"\n",
    "#         \"2. ANCHORING: If 'Step Three' mentions 'Permissions' and an image OCR contains 'Permissions requested', \"\n",
    "#         \"move that image URL to Step Three, regardless of where it appeared in the draft.\\n\"\n",
    "#         \"3. LOGICAL ORDERING: Ensure the final output follows a strict numerical sequence (Step 1, 2, 3, 4, 5, 6).\\n\"\n",
    "#         \"4. IMAGE PLACEMENT: Place the image(s) IMMEDIATELY after the step description.\\n\"\n",
    "#         \"5. FIDELITY: Maintain the SAS URLs exactly as provided in the manifest. Do not truncate or modify them.\\n\"\n",
    "#         \"6. DEDUPLICATION: Remove any redundant Step text or duplicate FAQ sections found in the draft.\"\n",
    "#     )\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b92fe-e522-4191-a938-e7250dbc8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_content = []\n",
    "\n",
    "# # 1. Lead with the User's Query\n",
    "# # This ensures the model knows the specific intent (e.g., \"Explain the steps\")\n",
    "# api_content.append({\n",
    "#     \"type\": \"input_text\", \n",
    "#     \"text\": f\"### USER QUERY / INTENT\\n{user_query}\\n\\n\"\n",
    "# })\n",
    "\n",
    "# # 2. Provide the Raw Interleaved Markdown\n",
    "# # This is the primary data source containing text and the SAS URLs\n",
    "# api_content.append({\n",
    "#     \"type\": \"input_text\", \n",
    "#     \"text\": f\"### SOURCE DOCUMENT (RAW MARKDOWN)\\n{llm_ready_markdown}\"\n",
    "# })\n",
    "\n",
    "# for url in vision_sas_urls:\n",
    "#     api_content.append({\n",
    "#         \"type\": \"input_image\",\n",
    "#         \"image_url\": url\n",
    "#     })\n",
    "\n",
    "# # 4. Final Instruction Reinforcement\n",
    "# # 4. Output constraints (not reasoning)\n",
    "# api_content.append({\n",
    "#     \"type\": \"input_text\",\n",
    "#     \"text\": (\n",
    "#         \"OUTPUT INSTRUCTIONS:\\n\"\n",
    "#         \"- Use both text and images to correctly order the steps.\\n\"\n",
    "#         \"- Preserve all SAS URLs exactly.\\n\"\n",
    "#         \"- Attach images to the correct step.\\n\"\n",
    "#         \"- Do not guess if an image is unclear.\\n\"\n",
    "#         \"- You must understand and reason the visual content in the image and try to interleave that images/images under respective step.\\n\"\n",
    "#         \"- Use the textual explanation as logical step to interleave repective image/images.\\n\"\n",
    "#     )\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e65b18-b3a5-45a2-987d-9e394fe443e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beaf038-c9d8-44eb-b448-3c0f963197d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc250bc-8520-482b-8c8c-9171a4796e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1304a4-da20-4290-a76d-a10fd25d3c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af247c-ed20-4ad3-b1da-9c6564cdb2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f3415-3694-4007-8954-6c2e7fada8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
